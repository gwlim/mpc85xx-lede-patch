diff --git a/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch b/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch
new file mode 100644
index 0000000..15d226f
--- /dev/null
+++ b/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch
@@ -0,0 +1,249 @@
+--- /dev/null	2017-07-22 09:51:22.066177025 +0800
++++ b/src/string/powerpc/memcpy.c	2017-07-20 19:56:14.000000000 +0800
+@@ -0,0 +1,82 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
++#include <string.h>
++
++/* PPC can do pre increment and load/store, but not post increment and
++   load/store.  Therefore use *++ptr instead of *ptr++.  */
++void *memcpy(void *to, const void *from, size_t len)
++{
++	unsigned long rem, chunks, tmp1, tmp2;
++	unsigned char *tmp_to;
++	unsigned char *tmp_from = (unsigned char *)from;
++
++	chunks = len / 8;
++	tmp_from -= 4;
++	tmp_to = to - 4;
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		/* make gcc to load all data, then store it */
++		tmp1 = *(unsigned long *)(tmp_from+4);
++		tmp_from += 8;
++		tmp2 = *(unsigned long *)tmp_from;
++		*(unsigned long *)(tmp_to+4) = tmp1;
++		tmp_to += 8;
++		*(unsigned long *)tmp_to = tmp2;
++	} while (--chunks);
++ lessthan8:
++	len = len % 8;
++	if (len >= 4) {
++		tmp_from += 4;
++		tmp_to += 4;
++		*(unsigned long *)(tmp_to) = *(unsigned long *)(tmp_from);
++		len -= 4;
++	}
++	if (!len)
++		return to;
++	tmp_from += 3;
++	tmp_to += 3;
++	do {
++		*++tmp_to = *++tmp_from;
++	} while (--len);
++
++	return to;
++ align:
++	/* ???: Do we really need to generate the carry flag here? If not, then:
++	rem -= 4; */
++	rem = 4 - rem;
++	len -= rem;
++	do {
++		*(tmp_to+4) = *(tmp_from+4);
++		++tmp_from;
++		++tmp_to;
++	} while (--rem);
++	chunks = len / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
++}
++libc_hidden_def(memcpy)
+--- /dev/null	2017-07-22 09:51:22.066177025 +0800
++++ b/src/string/powerpc/memmove.c	2017-07-20 19:56:14.000000000 +0800
+@@ -0,0 +1,76 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
++#include <string.h>
++
++
++void *memmove(void *to, const void *from, size_t n)
++{
++	unsigned long rem, chunks, tmp1, tmp2;
++	unsigned char *tmp_to;
++	unsigned char *tmp_from = (unsigned char *)from;
++
++	if (tmp_from >= (unsigned char *)to)
++		return memcpy(to, from, n);
++	chunks = n / 8;
++	tmp_from += n;
++	tmp_to = to + n;
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		/* make gcc to load all data, then store it */
++		tmp1 = *(unsigned long *)(tmp_from-4);
++		tmp_from -= 8;
++		tmp2 = *(unsigned long *)tmp_from;
++		*(unsigned long *)(tmp_to-4) = tmp1;
++		tmp_to -= 8;
++		*(unsigned long *)tmp_to = tmp2;
++	} while (--chunks);
++ lessthan8:
++	n = n % 8;
++	if (n >= 4) {
++		*(unsigned long *)(tmp_to-4) = *(unsigned long *)(tmp_from-4);
++		tmp_from -= 4;
++		tmp_to -= 4;
++		n = n-4;
++	}
++	if (!n ) return to;
++	do {
++		*--tmp_to = *--tmp_from;
++	} while (--n);
++
++	return to;
++ align:
++	rem = 4 - rem;
++	n = n - rem;
++	do {
++		*--tmp_to = *--tmp_from;
++	} while (--rem);
++	chunks = n / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
++}
++libc_hidden_def(memmove)
+--- /dev/null	2017-07-22 09:51:22.066177025 +0800
++++ b/src/string/powerpc/memset.c	2017-07-20 19:56:14.000000000 +0800
+@@ -0,0 +1,82 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
++#include <string.h>
++
++
++static __inline__ int expand_byte_word(int c){
++	/* this does:
++	   c = c << 8 | c;
++	   c = c << 16 | c ;
++	*/
++	__asm__("rlwimi	%0,%0,8,16,23\n"
++	    "\trlwimi	%0,%0,16,0,15\n"
++	    : "=r" (c) : "0" (c));
++	return c;
++}
++
++void *memset(void *to, int c, size_t n)
++{
++	unsigned long rem, chunks;
++	unsigned char *tmp_to;
++
++	chunks = n / 8;
++	tmp_to = to - 4;
++	c = expand_byte_word(c);
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++	} while (--chunks);
++ lessthan8:
++	n = n % 8;
++	if (n >= 4) {
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++		n = n-4;
++	}
++	if (!n ) return to;
++	tmp_to += 3;
++	do {
++		*++tmp_to = c;
++	} while (--n);
++
++	return to;
++ align:
++	rem = 4 - rem;
++	n = n-rem;
++	do {
++		*(tmp_to+4) = c;
++		++tmp_to;
++	} while (--rem);
++	chunks = n / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
++}
++libc_hidden_def(memset)
