diff --git a/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch b/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch
new file mode 100644
index 0000000..706acc7
--- /dev/null
+++ b/./toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch
@@ -0,0 +1,473 @@
+--- a/src/string/memcpy.c	2017-01-03 08:47:12.000000000 +0800
++++ b/src/string/memcpy.c	2017-07-22 12:30:27.000000000 +0800
+@@ -1,124 +1,83 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
+ #include <string.h>
+ #include <stdint.h>
+ #include <endian.h>
+ 
+-void *memcpy(void *restrict dest, const void *restrict src, size_t n)
++/* PPC can do pre increment and load/store, but not post increment and
++   load/store.  Therefore use *++ptr instead of *ptr++.  */
++void *memcpy(void *restrict to, const void *restrict from, size_t len)
+ {
+-	unsigned char *d = dest;
+-	const unsigned char *s = src;
+-
+-#ifdef __GNUC__
+-
+-#if __BYTE_ORDER == __LITTLE_ENDIAN
+-#define LS >>
+-#define RS <<
+-#else
+-#define LS <<
+-#define RS >>
+-#endif
+-
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	uint32_t w, x;
+-
+-	for (; (uintptr_t)s % 4 && n; n--) *d++ = *s++;
+-
+-	if ((uintptr_t)d % 4 == 0) {
+-		for (; n>=16; s+=16, d+=16, n-=16) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			*(u32 *)(d+8) = *(u32 *)(s+8);
+-			*(u32 *)(d+12) = *(u32 *)(s+12);
+-		}
+-		if (n&8) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			d += 8; s += 8;
+-		}
+-		if (n&4) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			d += 4; s += 4;
+-		}
+-		if (n&2) {
+-			*d++ = *s++; *d++ = *s++;
+-		}
+-		if (n&1) {
+-			*d = *s;
+-		}
+-		return dest;
+-	}
+-
+-	if (n >= 32) switch ((uintptr_t)d % 4) {
+-	case 1:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 3;
+-		for (; n>=17; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+1);
+-			*(u32 *)(d+0) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+5);
+-			*(u32 *)(d+4) = (x LS 24) | (w RS 8);
+-			x = *(u32 *)(s+9);
+-			*(u32 *)(d+8) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+13);
+-			*(u32 *)(d+12) = (x LS 24) | (w RS 8);
+-		}
+-		break;
+-	case 2:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 2;
+-		for (; n>=18; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+2);
+-			*(u32 *)(d+0) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+6);
+-			*(u32 *)(d+4) = (x LS 16) | (w RS 16);
+-			x = *(u32 *)(s+10);
+-			*(u32 *)(d+8) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+14);
+-			*(u32 *)(d+12) = (x LS 16) | (w RS 16);
+-		}
+-		break;
+-	case 3:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		n -= 1;
+-		for (; n>=19; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+3);
+-			*(u32 *)(d+0) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+7);
+-			*(u32 *)(d+4) = (x LS 8) | (w RS 24);
+-			x = *(u32 *)(s+11);
+-			*(u32 *)(d+8) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+15);
+-			*(u32 *)(d+12) = (x LS 8) | (w RS 24);
+-		}
+-		break;
+-	}
+-	if (n&16) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&8) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&4) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&2) {
+-		*d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&1) {
+-		*d = *s;
+-	}
+-	return dest;
+-#endif
+-
+-	for (; n; n--) *d++ = *s++;
+-	return dest;
++	unsigned long rem, chunks, tmp1, tmp2;
++	unsigned char *tmp_to = to;
++	const unsigned char *tmp_from = from;
++
++	chunks = len / 8;
++	tmp_from -= 4;
++	tmp_to -= 4;
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		/* make gcc to load all data, then store it */
++		tmp1 = *(unsigned long *)(tmp_from+4);
++		tmp_from += 8;
++		tmp2 = *(unsigned long *)tmp_from;
++		*(unsigned long *)(tmp_to+4) = tmp1;
++		tmp_to += 8;
++		*(unsigned long *)tmp_to = tmp2;
++	} while (--chunks);
++ lessthan8:
++	len = len % 8;
++	if (len >= 4) {
++		tmp_from += 4;
++		tmp_to += 4;
++		*(unsigned long *)(tmp_to) = *(unsigned long *)(tmp_from);
++		len -= 4;
++	}
++	if (!len)
++		return to;
++	tmp_from += 3;
++	tmp_to += 3;
++	do {
++		*++tmp_to = *++tmp_from;
++	} while (--len);
++
++	return to;
++ align:
++	/* ???: Do we really need to generate the carry flag here? If not, then:
++	rem -= 4; */
++	rem = 4 - rem;
++	len -= rem;
++	do {
++		*(tmp_to+4) = *(tmp_from+4);
++		++tmp_from;
++		++tmp_to;
++	} while (--rem);
++	chunks = len / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
+ }
+--- a/src/string/memmove.c	2017-07-22 09:51:22.066177025 +0800
++++ b/src/string/memmove.c	2017-07-20 19:56:14.000000000 +0800
+@@ -1,36 +1,78 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
+ #include <string.h>
+ #include <stdint.h>
+ 
+ #define WT size_t
+ #define WS (sizeof(WT))
+ 
+-void *memmove(void *dest, const void *src, size_t n)
++void *memmove(void *to, const void *from, size_t n)
+ {
+-	char *d = dest;
+-	const char *s = src;
+-
+-	if (d==s) return d;
+-	if (s+n <= d || d+n <= s) return memcpy(d, s, n);
++	unsigned long rem, chunks, tmp1, tmp2;
++	unsigned char *tmp_to = to;
++	unsigned char *tmp_from = (unsigned char *)from;
+ 
+-	if (d<s) {
+-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
+-			while ((uintptr_t)d % WS) {
+-				if (!n--) return dest;
+-				*d++ = *s++;
+-			}
+-			for (; n>=WS; n-=WS, d+=WS, s+=WS) *(WT *)d = *(WT *)s;
+-		}
+-		for (; n; n--) *d++ = *s++;
+-	} else {
+-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
+-			while ((uintptr_t)(d+n) % WS) {
+-				if (!n--) return dest;
+-				d[n] = s[n];
+-			}
+-			while (n>=WS) n-=WS, *(WT *)(d+n) = *(WT *)(s+n);
+-		}
+-		while (n) n--, d[n] = s[n];
++	if (tmp_from >= (unsigned char *)to)
++		return memcpy(to, from, n);
++	chunks = n / 8;
++	tmp_from += n;
++	tmp_to += n;
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		/* make gcc to load all data, then store it */
++		tmp1 = *(unsigned long *)(tmp_from-4);
++		tmp_from -= 8;
++		tmp2 = *(unsigned long *)tmp_from;
++		*(unsigned long *)(tmp_to-4) = tmp1;
++		tmp_to -= 8;
++		*(unsigned long *)tmp_to = tmp2;
++	} while (--chunks);
++ lessthan8:
++	n = n % 8;
++	if (n >= 4) {
++		*(unsigned long *)(tmp_to-4) = *(unsigned long *)(tmp_from-4);
++		tmp_from -= 4;
++		tmp_to -= 4;
++		n = n-4;
+ 	}
++	if (!n ) return to;
++	do {
++		*--tmp_to = *--tmp_from;
++	} while (--n);
+ 
+-	return dest;
++	return to;
++ align:
++	rem = 4 - rem;
++	n = n - rem;
++	do {
++		*--tmp_to = *--tmp_from;
++	} while (--rem);
++	chunks = n / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
+ }
+--- a/src/string/memset.c	2017-07-22 09:51:22.066177025 +0800
++++ b/src/string/memset.c	2017-07-20 19:56:14.000000000 +0800
+@@ -1,86 +1,81 @@
++/*
++ * Copyright (C) 2004 Joakim Tjernlund
++ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
++ *
++ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
++ */
++
++/* These are carefully optimized mem*() functions for PPC written in C.
++ * Don't muck around with these function without checking the generated
++ * assembler code.
++ * It is possible to optimize these significantly more by using specific
++ * data cache instructions(mainly dcbz). However that requires knownledge
++ * about the CPU's cache line size.
++ *
++ * BUG ALERT!
++ * The cache instructions on MPC8xx CPU's are buggy(they don't update
++ * the DAR register when causing a DTLB Miss/Error) and cannot be
++ * used on 8xx CPU's without a kernel patch to work around this
++ * problem.
++ */
++
+ #include <string.h>
+ #include <stdint.h>
+ 
+-void *memset(void *dest, int c, size_t n)
++static __inline__ int expand_byte_word(int c){
++	/* this does:
++	   c = c << 8 | c;
++	   c = c << 16 | c ;
++	*/
++	__asm__("rlwimi	%0,%0,8,16,23\n"
++	    "\trlwimi	%0,%0,16,0,15\n"
++	    : "=r" (c) : "0" (c));
++	return c;
++}
++
++void *memset(void *to, int c, size_t n)
+ {
+-	unsigned char *s = dest;
+-	size_t k;
++	unsigned long rem, chunks;
++	unsigned char *tmp_to = to;
+ 
+-	/* Fill head and tail with minimal branching. Each
+-	 * conditional ensures that all the subsequently used
+-	 * offsets are well-defined and in the dest region. */
+-
+-	if (!n) return dest;
+-	s[0] = s[n-1] = c;
+-	if (n <= 2) return dest;
+-	s[1] = s[n-2] = c;
+-	s[2] = s[n-3] = c;
+-	if (n <= 6) return dest;
+-	s[3] = s[n-4] = c;
+-	if (n <= 8) return dest;
+-
+-	/* Advance pointer to align it at a 4-byte boundary,
+-	 * and truncate n to a multiple of 4. The previous code
+-	 * already took care of any head/tail that get cut off
+-	 * by the alignment. */
+-
+-	k = -(uintptr_t)s & 3;
+-	s += k;
+-	n -= k;
+-	n &= -4;
+-
+-#ifdef __GNUC__
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	typedef uint64_t __attribute__((__may_alias__)) u64;
+-
+-	u32 c32 = ((u32)-1)/255 * (unsigned char)c;
+-
+-	/* In preparation to copy 32 bytes at a time, aligned on
+-	 * an 8-byte bounary, fill head/tail up to 28 bytes each.
+-	 * As in the initial byte-based head/tail fill, each
+-	 * conditional below ensures that the subsequent offsets
+-	 * are valid (e.g. !(n<=24) implies n>=28). */
+-
+-	*(u32 *)(s+0) = c32;
+-	*(u32 *)(s+n-4) = c32;
+-	if (n <= 8) return dest;
+-	*(u32 *)(s+4) = c32;
+-	*(u32 *)(s+8) = c32;
+-	*(u32 *)(s+n-12) = c32;
+-	*(u32 *)(s+n-8) = c32;
+-	if (n <= 24) return dest;
+-	*(u32 *)(s+12) = c32;
+-	*(u32 *)(s+16) = c32;
+-	*(u32 *)(s+20) = c32;
+-	*(u32 *)(s+24) = c32;
+-	*(u32 *)(s+n-28) = c32;
+-	*(u32 *)(s+n-24) = c32;
+-	*(u32 *)(s+n-20) = c32;
+-	*(u32 *)(s+n-16) = c32;
+-
+-	/* Align to a multiple of 8 so we can fill 64 bits at a time,
+-	 * and avoid writing the same bytes twice as much as is
+-	 * practical without introducing additional branching. */
+-
+-	k = 24 + ((uintptr_t)s & 4);
+-	s += k;
+-	n -= k;
+-
+-	/* If this loop is reached, 28 tail bytes have already been
+-	 * filled, so any remainder when n drops below 32 can be
+-	 * safely ignored. */
+-
+-	u64 c64 = c32 | ((u64)c32 << 32);
+-	for (; n >= 32; n-=32, s+=32) {
+-		*(u64 *)(s+0) = c64;
+-		*(u64 *)(s+8) = c64;
+-		*(u64 *)(s+16) = c64;
+-		*(u64 *)(s+24) = c64;
++	chunks = n / 8;
++	tmp_to -= 4;
++	c = expand_byte_word(c);
++	if (!chunks)
++		goto lessthan8;
++	rem = (unsigned long )tmp_to % 4;
++	if (rem)
++		goto align;
++ copy_chunks:
++	do {
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++	} while (--chunks);
++ lessthan8:
++	n = n % 8;
++	if (n >= 4) {
++		*(unsigned long *)(tmp_to+4) = c;
++		tmp_to += 4;
++		n = n-4;
+ 	}
+-#else
+-	/* Pure C fallback with no aliasing violations. */
+-	for (; n; n--, s++) *s = c;
+-#endif
+-
+-	return dest;
++	if (!n ) return to;
++	tmp_to += 3;
++	do {
++		*++tmp_to = c;
++	} while (--n);
++
++	return to;
++ align:
++	rem = 4 - rem;
++	n = n-rem;
++	do {
++		*(tmp_to+4) = c;
++		++tmp_to;
++	} while (--rem);
++	chunks = n / 8;
++	if (chunks)
++		goto copy_chunks;
++	goto lessthan8;
+ }
\ No newline at end of file
